# -*- coding: utf-8 -*-
"""data_fetcher.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1wSakcgcXg6NHus7I-5YL6J16kPZh4gMb
"""

!pip install earthengine-api geemap pandas
!earthengine authenticate
import ee
import geemap
import pandas as pd
import os

ee.Initialize()

import ee
import geemap
import pandas as pd
import os
from tqdm import tqdm
import time
import json
from concurrent.futures import ThreadPoolExecutor, as_completed
import threading

# ============================================================
# CONFIGURATION
# ============================================================
SAVE_DIR = "/content/drive/MyDrive/train_naip_images"
CHECKPOINT_FILE = "/content/drive/MyDrive/download_progress.json"
BATCH_SIZE = 500
BUFFER_M = 80
SCALE = 1.0
NUM_WORKERS = 6  # Parallel threads

os.makedirs(SAVE_DIR, exist_ok=True)

# Thread-safe lock for checkpoint updates
checkpoint_lock = threading.Lock()

# 1. BUILD NAIP MOSAIC ONCE

df = pd.read_excel("/content/drive/MyDrive/train_cleaned.xlsx")

print("Building NAIP mosaic for entire King County...")
points = [ee.Geometry.Point([lon, lat])
          for lon, lat in zip(df["long"].head(1000), df["lat"].head(1000))]
roi_all = ee.Geometry.MultiPoint(points).convexHull(5000)

naip_col = (ee.ImageCollection("USDA/NAIP/DOQQ")
            .filterBounds(roi_all)
            .filterDate("2014-01-01", "2016-01-01"))

count = naip_col.size().getInfo()
print(f"Found {count} NAIP images in collection")

if count == 0:
    raise ValueError("No NAIP coverage for 2014-2016 in King County")

naip_mosaic = naip_col.median().select(["R", "G", "B"])
print("‚úÖ Mosaic built. Ready to export.\n")

# 2. THREAD-SAFE CHECKPOINT SYSTEM

def load_checkpoint():

    if os.path.exists(CHECKPOINT_FILE):
        with open(CHECKPOINT_FILE, 'r') as f:
            return set(json.load(f))
    return set()

def save_checkpoint(completed_ids):

    with checkpoint_lock:
        with open(CHECKPOINT_FILE, 'w') as f:
            json.dump(list(completed_ids), f)

completed = load_checkpoint()
print(f"Already completed: {len(completed)} houses")

# 3. PARALLEL-SAFE EXPORT FUNCTION

def export_house_image(row, save_dir=SAVE_DIR, buffer_m=BUFFER_M, scale=SCALE):

    pid = int(row["id"])
    lat = row["lat"]
    lon = row["long"]

    filename = f"{save_dir}/house_{pid}.tif"

    # Skip if already exists
    if os.path.exists(filename):
        return (pid, True, "already_exists")

    try:
        point = ee.Geometry.Point([lon, lat])
        region = point.buffer(buffer_m)
        img = naip_mosaic.clip(region)

        geemap.ee_export_image(
            img,
            filename=filename,
            scale=scale,
            region=region,
            file_per_band=False,
            timeout=300
        )
        return (pid, True, None)

    except Exception as e:
        return (pid, False, str(e)[:100])

# 4. PARALLEL BATCH PROCESSING

failed = []
batch_num = 0

remaining_df = df[~df["id"].isin(completed)].reset_index(drop=True)
print(f"Remaining to download: {len(remaining_df)} houses")
print(f"Using {NUM_WORKERS} parallel workers\n")

for start_idx in range(0, len(remaining_df), BATCH_SIZE):
    batch_num += 1
    end_idx = min(start_idx + BATCH_SIZE, len(remaining_df))
    batch_df = remaining_df.iloc[start_idx:end_idx]

    print(f"\n{'='*60}")
    print(f"BATCH {batch_num}: Houses {start_idx} to {end_idx-1}")
    print(f"{'='*60}")

    batch_start = time.time()
    batch_completed = []
    batch_failed = []

    # Parallel processing with ThreadPoolExecutor
    with ThreadPoolExecutor(max_workers=NUM_WORKERS) as executor:

        future_to_row = {
            executor.submit(export_house_image, row): row
            for _, row in batch_df.iterrows()
        }

        # Progress bar
        with tqdm(total=len(batch_df), desc=f"Batch {batch_num}") as pbar:
            for future in as_completed(future_to_row):
                pid, success, msg = future.result()

                if success:
                    batch_completed.append(pid)
                    if msg != "already_exists":
                        print(f"‚úÖ house_{pid}")
                else:
                    batch_failed.append(pid)
                    print(f"‚ùå house_{pid}: {msg}")

                pbar.update(1)

    # Update global tracking
    completed.update(batch_completed)
    failed.extend(batch_failed)
    save_checkpoint(completed)

    batch_time = time.time() - batch_start
    avg_per_image = batch_time / len(batch_df)

    print(f"\nBatch {batch_num} summary:")
    print(f"  ‚úÖ Success: {len(batch_completed)}")
    print(f"  ‚ùå Failed: {len(batch_failed)}")
    print(f"  ‚è±Ô∏è  Time: {batch_time/60:.1f} min ({avg_per_image:.1f}s/image)")
    print(f"  üöÄ Speedup: ~{NUM_WORKERS}√ó vs sequential")
    print(f"  üíæ Total completed: {len(completed)}/{len(df)}")

    # Estimate remaining time
    remaining = len(df) - len(completed)
    est_hours = (remaining * avg_per_image) / 3600
    print(f"  ‚è≥ Est. remaining: {est_hours:.1f} hours")

# 5. FINAL SUMMARY

print("\n" + "="*60)
print("DOWNLOAD COMPLETE")
print("="*60)
print(f"‚úÖ Total successful: {len(completed)}")
print(f"‚ùå Total failed: {len(failed)}")

if failed:
    print(f"\nFailed IDs saved to 'failed_downloads.json'")
    with open("failed_downloads.json", "w") as f:
        json.dump(failed, f)

print(f"\nüìÅ Images saved to: {SAVE_DIR}/")
print(f"üíæ Checkpoint saved to: {CHECKPOINT_FILE}")






"""**FOR TEST DATASET**"""

import ee
import geemap
import pandas as pd
import os
from tqdm import tqdm
import time
import json
from concurrent.futures import ThreadPoolExecutor, as_completed
import threading

# CONFIGURATION

SAVE_DIR = "/content/drive/MyDrive/test_naip_images"
CHECKPOINT_FILE = "/content/drive/MyDrive/download_progress.json"
BATCH_SIZE = 500
BUFFER_M = 80
SCALE = 1.0
NUM_WORKERS = 6  # Parallel threads

os.makedirs(SAVE_DIR, exist_ok=True)

# Thread-safe lock for checkpoint updates
checkpoint_lock = threading.Lock()

# 1. BUILD NAIP MOSAIC ONCE

df = pd.read_excel("/content/drive/MyDrive/test2.xlsx")

print("Building NAIP mosaic for entire King County...")
points = [ee.Geometry.Point([lon, lat])
          for lon, lat in zip(df["long"].head(1000), df["lat"].head(1000))]
roi_all = ee.Geometry.MultiPoint(points).convexHull(5000)

naip_col = (ee.ImageCollection("USDA/NAIP/DOQQ")
            .filterBounds(roi_all)
            .filterDate("2014-01-01", "2016-01-01"))

count = naip_col.size().getInfo()
print(f"Found {count} NAIP images in collection")

if count == 0:
    raise ValueError("No NAIP coverage for 2014-2016 in King County")

naip_mosaic = naip_col.median().select(["R", "G", "B"])
print("‚úÖ Mosaic built. Ready to export.\n")

# 2. THREAD-SAFE CHECKPOINT SYSTEM

def load_checkpoint():

    if os.path.exists(CHECKPOINT_FILE):
        with open(CHECKPOINT_FILE, 'r') as f:
            return set(json.load(f))
    return set()

def save_checkpoint(completed_ids):

    with checkpoint_lock:
        with open(CHECKPOINT_FILE, 'w') as f:
            json.dump(list(completed_ids), f)

completed = load_checkpoint()
print(f"Already completed: {len(completed)} houses")

# 3. PARALLEL-SAFE EXPORT FUNCTION

def export_house_image(row, save_dir=SAVE_DIR, buffer_m=BUFFER_M, scale=SCALE):

    pid = int(row["id"])
    lat = row["lat"]
    lon = row["long"]

    filename = f"{save_dir}/house_{pid}.tif"

    # Skip if already exists
    if os.path.exists(filename):
        return (pid, True, "already_exists")

    try:
        point = ee.Geometry.Point([lon, lat])
        region = point.buffer(buffer_m)
        img = naip_mosaic.clip(region)

        geemap.ee_export_image(
            img,
            filename=filename,
            scale=scale,
            region=region,
            file_per_band=False,
            timeout=300
        )
        return (pid, True, None)

    except Exception as e:
        return (pid, False, str(e)[:100])

# 4. PARALLEL BATCH PROCESSING

failed = []
batch_num = 0

# Filter out already completed
remaining_df = df[~df["id"].isin(completed)].reset_index(drop=True)
print(f"Remaining to download: {len(remaining_df)} houses")
print(f"Using {NUM_WORKERS} parallel workers\n")

for start_idx in range(0, len(remaining_df), BATCH_SIZE):
    batch_num += 1
    end_idx = min(start_idx + BATCH_SIZE, len(remaining_df))
    batch_df = remaining_df.iloc[start_idx:end_idx]

    print(f"\n{'='*60}")
    print(f"BATCH {batch_num}: Houses {start_idx} to {end_idx-1}")
    print(f"{'='*60}")

    batch_start = time.time()
    batch_completed = []
    batch_failed = []

    # Parallel processing with ThreadPoolExecutor
    with ThreadPoolExecutor(max_workers=NUM_WORKERS) as executor:

        future_to_row = {
            executor.submit(export_house_image, row): row
            for _, row in batch_df.iterrows()
        }

        # Progress bar
        with tqdm(total=len(batch_df), desc=f"Batch {batch_num}") as pbar:
            for future in as_completed(future_to_row):
                pid, success, msg = future.result()

                if success:
                    batch_completed.append(pid)
                    if msg != "already_exists":
                        print(f"‚úÖ house_{pid}")
                else:
                    batch_failed.append(pid)
                    print(f"‚ùå house_{pid}: {msg}")

                pbar.update(1)

    # Update global tracking
    completed.update(batch_completed)
    failed.extend(batch_failed)
    save_checkpoint(completed)

    batch_time = time.time() - batch_start
    avg_per_image = batch_time / len(batch_df)

    print(f"\nBatch {batch_num} summary:")
    print(f"  ‚úÖ Success: {len(batch_completed)}")
    print(f"  ‚ùå Failed: {len(batch_failed)}")
    print(f"  ‚è±Ô∏è  Time: {batch_time/60:.1f} min ({avg_per_image:.1f}s/image)")
    print(f"  üöÄ Speedup: ~{NUM_WORKERS}√ó vs sequential")
    print(f"  üíæ Total completed: {len(completed)}/{len(df)}")

    # Estimate remaining time
    remaining = len(df) - len(completed)
    est_hours = (remaining * avg_per_image) / 3600
    print(f"  ‚è≥ Est. remaining: {est_hours:.1f} hours")

# FINAL SUMMARY

print("\n" + "="*60)
print("DOWNLOAD COMPLETE")
print("="*60)
print(f"‚úÖ Total successful: {len(completed)}")
print(f"‚ùå Total failed: {len(failed)}")

if failed:
    print(f"\nFailed IDs saved to 'failed_downloads.json'")
    with open("failed_downloads.json", "w") as f:
        json.dump(failed, f)

print(f"\nüìÅ Images saved to: {SAVE_DIR}/")
print(f"üíæ Checkpoint saved to: {CHECKPOINT_FILE}")







"""**IMAGE ALIGNING**

**TRAIN IMAGES ALIGNING**
"""

import os
import pandas as pd

DATA_PATH = "/content/drive/MyDrive/train_cleaned.xlsx"
IMAGE_DIR = "/content/drive/MyDrive/train_naip_images"
IMAGE_EXT = ".tif"

df = pd.read_excel(DATA_PATH)

dataset_ids = set(df["id"].astype(int))

print("Total rows in dataset:", len(dataset_ids))

image_ids = set(
    int(f.replace("house_", "").replace(IMAGE_EXT, ""))
    for f in os.listdir(IMAGE_DIR)
    if f.startswith("house_") and f.endswith(IMAGE_EXT)
)

print("Total images found:", len(image_ids))

missing_images = dataset_ids - image_ids
print("Missing images:", len(missing_images))

pd.DataFrame({"missing_id": list(missing_images)}).to_csv(
    "/content/drive/MyDrive/missing_imagess_ids.csv",
    index=False
)

print("Missing_imagess_ids.csv saved")

df["image_exists"] = df["id"].isin(image_ids)
df["tif_path"] = "/content/drive/MyDrive/train_naip_images/"
df["image_path"] = df["id"].apply(
    lambda x: f"house_{x}{IMAGE_EXT}" if x in image_ids else None
)

df.to_excel("/content/drive/MyDrive/train_with_images_status.xlsx", index=False)
print("‚úÖ Final aligned dataset saved")

"""**TEST IMAGES ALIGNING**"""

import os
import pandas as pd

DATA_PATH_TEST = "/content/drive/MyDrive/test_1.xlsx"
IMAGE_DIR_TEST = "/content/drive/MyDrive/test_naip_images"
IMAGE_EXT = ".tif"

df = pd.read_excel(DATA_PATH_TEST)

test_dataset_ids = set(df["id"].astype(int))

print("Total rows in dataset:", len(test_dataset_ids))

image_ids = set(
    int(f.replace("house_", "").replace(IMAGE_EXT,""))
    for f in os.listdir(IMAGE_DIR_TEST)
    if f.startswith("house_") and f.endswith(IMAGE_EXT)
)

print("Total images found:", len(image_ids))

missing_images_test = test_dataset_ids - image_ids
print("Missing images:", len(missing_images_test))

pd.DataFrame({"missing_id": list(missing_images_test)}).to_csv(
    "/content/drive/MyDrive/missing_imagess_test_ids.csv",
    index=False
)

print("Missing_imagess_ids.csv saved")

df["image_exists"] = df["id"].isin(image_ids)
df["tif_path"] = "/content/drive/MyDrive/test_naip_images/"
df["image_path"] = df["id"].apply(
    lambda x: f"house_{x}{IMAGE_EXT}" if x in image_ids else None
)

df.to_excel("/content/drive/MyDrive/testdata_process.xlsx", index=False)
print("Final aligned dataset saved")